scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
detach(tidyverse)
library(pacman)
detach(tidyverse)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
require(ggplot2)
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
require(ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
#ggplot2 version?
devtools::install_github("tidyverse/ggplot2")
setwd("~/abstweets")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#files for shiny app tables
saveRDS(useractivity_vg,"activity_gsg.rds")
saveRDS(userretweets_vg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
nobillag <- readRDS("allTweets_def.Rds")
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
nobillag <- readRDS("allTweets_def.Rds")
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
View(nobillag)
nobillag <- readRDS("allTweets_def.Rds")
setwd("~/abstweets_cron")
setwd("~/abstweets_cron")
setwd("~/abstweets_cron")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#files for shiny app tables
saveRDS(useractivity_vg,"activity_gsg.rds")
saveRDS(userretweets_vg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
nobillag <- readRDS("allTweets_def.Rds")
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
saveRDS(nobi,"nobillagbaseline.RDS")
setwd("~/abstweets")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#files for shiny app tables
saveRDS(useractivity_vg,"activity_gsg.rds")
saveRDS(userretweets_vg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
# nobillag <- readRDS("allTweets_def.Rds")
#
# colnames(nobillag)<- c("screen_name","text","link","date")
#
# nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#
# #berechne countdown für Nobillag
# nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
#                                 group_by(dmynb) %>%
#                                 summarize(nb=n()) %>%
#                                 mutate(countdown=as.Date("2018-03-04")-dmynb)
# saveRDS(nobi,"nobillagbaseline.RDS")
nobi <-readRDS("nobillagbaseline.RDS")
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
#files for shiny app tables
saveRDS(useractivity_gsg,"activity_gsg.rds")
saveRDS(userretweets_gsg,"retweets_gsg.rds")
runApp()
#sheets which contains gsg_mu in title
sd <-gs_ls("Sozialdetektive")
#save google object ref
sd <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_sd<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_sd<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_sd<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_sd,"activity_sd.rds")
saveRDS(userretweets_sd,"retweets_sd.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_sd.csv")
#sheets which contains gsg_mu in title
sd <-gs_ls("Sozialdetektive")
#save google object ref
sd <-map(sd$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_sd<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_sd<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_sd<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_sd,"activity_sd.rds")
saveRDS(userretweets_sd,"retweets_sd.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_sd.csv")
runApp()
