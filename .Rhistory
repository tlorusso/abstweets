# glimpse(rstats)
#
# filter(rstats, retweet_count > 0) %>%
#   select(text, mentions_screen_name, retweet_count) %>%
#   mutate(text = substr(text, 1, 30)) %>%
#   unnest()
#
#
# filter(rstats, str_detect(text, "(RT|via)((?:[[:blank:]:]\\W*@\\w+)+)")) %>%
#   select(text, mentions_screen_name, retweet_count) %>%
#   mutate(extracted = str_match(text, "(RT|via)((?:[[:blank:]:]\\W*@\\w+)+)")[,3]) %>%
#   mutate(text = substr(text, 1, 30)) %>%
#   unnest()
filter(rstats, retweet_count > 5) %>%
select(screen_name, mentions_screen_name) %>%
unnest(mentions_screen_name) %>%
filter(!is.na(mentions_screen_name)) %>%
graph_from_data_frame() -> rt_g
ggplot(data_frame(y=degree_distribution(rt_g), x=1:length(y))) +
geom_segment(aes(x, y, xend=x, yend=0), color="slateblue") +
scale_y_continuous(expand=c(0,0), trans="sqrt") +
labs(x="Degree", y="Density (sqrt scale)", title="#rstats Retweet Degree Distribution")
# theme_ipsum_rc(grid="Y", axis="x")
V(rt_g)$node_label <- unname(ifelse(degree(rt_g)[V(rt_g)] > 25, names(V(rt_g)), ""))
V(rt_g)$node_size <- unname(ifelse(degree(rt_g)[V(rt_g)] > 25, degree(rt_g), 0))
# gg1 <-
ggraph(rt_g, layout = 'linear', circular = TRUE) +
geom_edge_arc(edge_width=0.0755, aes(alpha=..index..)) +
geom_node_label(aes(label=node_label, size=node_size),
label.size=0, fill="#ffffff66", segment.colour="springgreen",
color="slateblue", repel=TRUE, fontface="bold") +
coord_fixed() +
scale_size_area(trans="sqrt") +
labs(title="Retweet Relationships", subtitle="Namen mit den meisten Retweets gelabelt.\nDarkers edges == more retweets. Node size == larger degree") +
theme_graph(base_family="raleway") +
theme(legend.position="none")
# gg1
ggsave("graph.png", units="cm",width = 20, height = 20)
#
install.packages("keras")
# Load in the keras package
library(keras)
# Install TensorFlow
install_tensorflow()
# Load in the keras package
library(keras)
# Install TensorFlow
install_tensorflow()
# Klassifizierung der User nach Pro / Kontra funktioniert für die Top-Twitterer relativ gut
userkontra <-tweetpredictall %>% group_by(user,predcat) %>% summarize(anteil=n()/sum(n())) %>% arrange(desc(anteil))
library(rtweet)
library(rtweet)
nobillag <- search_tweets("#nobillag", n=5000)
neinzunobillag <- search_tweets("#neinzunobillag", n=5000)
nonobillag <- search_tweets("#nonobillag", n=5000)
nobillag_date <-bind_rowds(nobillag,neinzunobillag,nonobillag) %>% distinct(status_id,keep_all=T)
saveRDS(nobillag_date,"nobillag_11022018.RDS")
blogdown::new_site(theme="digitalcraftsman/hugo-minimalist-theme")
blogdown::new_site(theme="digitalcraftsman/hugo-minimalist-theme")
shiny::runApp('abstweets/example')
runApp('abstweets')
devtools::install_github("JohnCoene/echarts")
devtools::install_github("JohnCoene/echarts4r")
shiny::runApp('abstweets')
runApp('abstweets')
runApp('abstweets')
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
devtools::install_github("tidyverse/ggplot2")
devtools::install_github("tidyverse/ggplot2")
require(ggplot2)
install.packages("rlang")
install.packages("rlang")
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
# Dataset (Motorization and "generalabonnement"-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#merge data with shapefile
mapdata <- merge(shape2, mfz_ga, by.x="id", by.y="bfs",all.x = TRUE, all.y = TRUE)
mapdata <- mapdata[order(mapdata$order),]
#how does the df look like?
mapdata[11300:11310,]
# ggplot mapping  # data layer
ggplot(data=mapdata,aes(x=long, y=lat, group=group)) +
geom_path(color='gray') +
coord_equal()+
geom_polygon(aes(fill=Anteil_GA),color = "white")+
theme_void()+
theme(legend.key.size = unit(1,"line"),
#Grösse Legende
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
sfmap
library(gghighlight)
#library(devtools)
#install_github(statistikZH/statR)
library(statR)
#library(devtools)
install_github(statistikZH/statR)
#library(devtools)
devtools::install_github(statistikZH/statR)
#library(devtools)
devtools::install_github("statistikZH/statR")
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
# Dataset (Motorization and "generalabonnement"-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#merge data with shapefile
mapdata <- merge(shape2, mfz_ga, by.x="id", by.y="bfs",all.x = TRUE, all.y = TRUE)
mapdata <- mapdata[order(mapdata$order),]
#how does the df look like?
mapdata[11300:11310,]
# ggplot mapping  # data layer
ggplot(data=mapdata,aes(x=long, y=lat, group=group)) +
geom_path(color='gray') +
coord_equal()+
geom_polygon(aes(fill=Anteil_GA),color = "white")+
theme_void()+
theme(legend.key.size = unit(1,"line"),
#Grösse Legende
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
sfmap
library(gghighlight)
#library(devtools)
# devtools::install_github("statistikZH/statR")
# library(statR)
gghighlight_point(gemdata, aes(mfzpro100ew,Anteil_GA),Anteil_GA > 7 & mfzpro100ew<600 |
mfzpro100ew>1000|
mfzpro100ew<700 & Anteil_GA<3|
mfzpro100ew>750 & Anteil_GA>9)+
geom_point(aes(size=Anzahl_Einw/1000, color=Anteil_HTA))+
# theme_stat()+
scale_size(name="Einwohner")+
labs(title="Motorisierungsgrad & Generalabonnements pro Gemeinde\n",
y="Anzahl GA pro 100 Einwohner",x="Motorfahrzeuge pro 1000 Einwohner")
# Shapefiles of the "Handlungsräume & Bauzonen"
handlungsraum<- st_read("geodata/handlungsraeume", stringsAsFactors = FALSE)
bauzonen <- st_read("geodata/bauzonen", stringsAsFactors = FALSE)
sfmap+
geom_sf(data=handlungsraum,fill=NA)+
geom_sf(data=bauzonen,alpha=0.5,color="white")+
coord_sf(datum = NA)
# #CRS CH LV95 -> swiss projection
# st_crs(gemeinden)<- "+init=epsg:2056"
#Projektion wgs 84 lat lng
# sbb <- st_transform(sbb,"+init=epsg:4326 +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +units=m +no_defs")
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
?fortify
??fortify
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- ggplot2::fortify(shape, region='BFS')
library(pacman)
pacman::p_load(rgdal, rgeos,tidyverse)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
# Dataset (Motorization and "generalabonnement"-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#merge data with shapefile
mapdata <- merge(shape2, mfz_ga, by.x="id", by.y="bfs",all.x = TRUE, all.y = TRUE)
mapdata <- mapdata[order(mapdata$order),]
#how does the df look like?
mapdata[11300:11310,]
# ggplot mapping  # data layer
ggplot(data=mapdata,aes(x=long, y=lat, group=group)) +
geom_path(color='gray') +
coord_equal()+
geom_polygon(aes(fill=Anteil_GA),color = "white")+
theme_void()+
theme(legend.key.size = unit(1,"line"),
#Grösse Legende
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
detach(tidyverse)
library(pacman)
detach(tidyverse)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
require(ggplot2)
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
require(ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton Zürich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
#ggplot2 version?
devtools::install_github("tidyverse/ggplot2")
shiny::runApp('abstweets')
shiny::runApp('abstweets')
setwd("~/abstweets")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which contains gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#files for shiny app tables
saveRDS(useractivity_gsg,"activity_gsg.rds")
saveRDS(userretweets_gsg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
# auskommentiert: im abstweets ordner (github) liegt die datei mit berechneter baseline!
# nobillag <- readRDS("allTweets_def.Rds")
#
# colnames(nobillag)<- c("screen_name","text","link","date")
#
# nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#
# #berechne countdown für Nobillag
# nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
#                                 group_by(dmynb) %>%
#                                 summarize(nb=n()) %>%
#                                 mutate(countdown=as.Date("2018-03-04")-dmynb)
# saveRDS(nobi,"nobillagbaseline.RDS")
nobi <-readRDS("nobillagbaseline.RDS")
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
# Überwachung von Versicherten ----------------------
#sheets which contains gsg_mu in title
sd <-gs_ls("Sozialdetektive")
#save google object ref
sd <-map(sd$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_sd<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_sd<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_sd<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_sd,"activity_sd.rds")
saveRDS(userretweets_sd,"retweets_sd.rds")
saveRDS(tweets_n_sd,"tweets_sd.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_sd.csv")
runApp()
pacman::p_load(tuber,tidyverse,stringr)
yt_oauth("th.lorusso@gmail.com",
"61z7CivkNHnY",token = "")
httr::set_config( config( ssl_verifypeer = 0L ) ) # = Fixes some certificate problems on linux = #
# = Autentication = #
yt_oauth("th.lorusso@gmail.com",
"61z7CivkNHnY",token = "")
yt_oauth("998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com", "AIzaSyCQirfPu7Kh_d5qaRzzHYW2D28UzLIvX4M")
res <- yt_search("Geldspielgesetz")
httr::set_config( config( ssl_verifypeer = 0L ) ) # = Fixes some certificate problems on linux = #
yt_oauth("998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com", "AIzaSyCQirfPu7Kh_d5qaRzzHYW2D28UzLIvX4M")
# = Download and prepare data = #
res <- yt_search("Geldspielgesetz")
yt_oauth("998136489867-5t3tq1g7hbovoj46dreqd6k5kd35ctjn.apps.googleusercontent.com", "
AIzaSyBeaeH3CCaeUphcBrBXp5s3czBRBGWskiw")
res <- yt_search("Geldspielgesetz")
