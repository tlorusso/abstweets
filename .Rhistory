)
#detect language to filter tweets
tweets2<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(hashtags %in% corrupt_ht)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
hashtags %in% corrupt_ht & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
hashtags %in% corrupt_ht & cld3 %in% c("de","fr","it")
)
#detect language to filter tweets
tweets2<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
View(tweets2)
tweets %>% anti_join(tweets)
tweets %>% anti_join(tweets2)
anti<-tweets %>% anti_join(tweets2)
View(anti)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
head(tweets)
setwd("~/abstweets_cron")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
tweets <- vg %>% gs_read()
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
saveRDS(useractivity,"abstweets/activity.rds")
saveRDS(userretweets,"abstweets/retweets.rds")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets2<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
# %>%
#   gather(vorlage,anzahl,-dmy)
saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
nobillag <- readRDS("allTweets_def.Rds")
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
vg_gsg <-vg_gsg %>% left_join(nobi)
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
tweets <- vg %>% gs_read()
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
saveRDS(useractivity,"abstweets/activity.rds")
saveRDS(userretweets,"abstweets/retweets.rds")
#Meiste Retweets
userretweets<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets2<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
nobillag <- readRDS("allTweets_def.Rds")
View(nobillag)
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
vg_gsg <-vg_gsg %>% left_join(nobi)
View(vgi)
View(userretweets_vg)
View(vg_gsg)
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
# %>%
setwd("~/abstweets_cron")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app visualisations
saveRDS(useractivity_vg,"abstweets/activity.rds")
saveRDS(userretweets_vg,"abstweets/retweets.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"abstweets/rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#single tweets for raw-data download
write.csv(tweets,"abstweets/rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
nobillag <- readRDS("allTweets_def.Rds")
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi)
saveRDS(vg_gsg,"abstweets/vg_gsg.rds")
setwd("~/abstweets_cron")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#für datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which containt gsg_mu in title
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app visualisations
saveRDS(useractivity_vg,"abstweets/activity.rds")
saveRDS(userretweets_vg,"abstweets/retweets.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"abstweets/rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#single tweets for raw-data download
write.csv(tweets,"abstweets/rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
nobillag <- readRDS("allTweets_def.Rds")
colnames(nobillag)<- c("screen_name","text","link","date")
nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#berechne countdown für Nobillag
nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
group_by(dmynb) %>%
summarize(nb=n()) %>%
mutate(countdown=as.Date("2018-03-04")-dmynb)
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi)
saveRDS(vg_gsg,"abstweets/vg_gsg.rds")
setwd("~/abstweets")
#files for shiny app visualisations
saveRDS(useractivity_vg,"abstweets/activity.rds")
#files for shiny app visualisations
saveRDS(useractivity_vg,"activity.rds")
#files for shiny app visualisations
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"abstweets/rawdata_vg.csv")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
#files for shiny app tables
saveRDS(useractivity_vg,"activity_gsg.rds")
saveRDS(userretweets_vg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
(vg_gsg,"vg_gsg.rds")
saveRDS(vg_gsg,"vg_gsg.rds")
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
shiny::runApp()
