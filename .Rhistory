<<<<<<< Updated upstream
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
library(wordcloud2)
library(tm)
library(wordcloud)
library(SnowballC)
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
library(pacman)
p_load(tidyverse,igraph,ggraph,rtweet,hrbrthemes)
rstats <-readRDS("rstatssrf1301.RDS")
library(wordcloud2)
library(tm)
library(wordcloud)
library(SnowballC)
library("textcat")
library("stringr")
library(tidyverse)
rstats$clean_tweet = gsub("&amp", "", rstats$text)
rstats$clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("@\\w+", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[[:punct:]]", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[[:digit:]]", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("http\\w+", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[ \t]{2,}", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("^\\s+|\\s+$", "", rstats$clean_tweet)
#welche sprache?
rstats$lang <- unlist(textcat(rstats$clean_tweet))
rstats <- rstats %>% filter(lang=="german")
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
rstats$clean_tweet<- iconv(rstats$clean_tweet, "ASCII", "UTF-8", sub="")
View(rstats)
jeopCorpus[["content"]]
rstats <-readRDS("rstatssrf1301.RDS")
rstats$clean_tweet = gsub("@\\w+", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[[:punct:]]", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[[:digit:]]", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", rstats$clean_tweet)
#welche sprache?
rstats$lang <- unlist(textcat(rstats$clean_tweet))
rstats <- rstats %>% filter(lang=="german")
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet))
dtm <- TermDocumentMatrix(jeopCorpus)
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
jeopCorpus$`4`
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(content_transformer(tolower))
rstats$clean_tweet = gsub("&amp", "", rstats$text)
rstats$clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("@\\w+", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[[:punct:]]", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[[:digit:]]", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("http\\w+", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("[ \t]{2,}", "", rstats$clean_tweet)
rstats$clean_tweet = gsub("^\\s+|\\s+$", "", rstats$clean_tweet)
View(rstats)
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet))
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(content_transformer(tolower))
jeopCorpus[[1]]$content
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(content_transformer(tolower))
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
jeopCorpus[[1]]$content
dtm <- TermDocumentMatrix(jeopCorpus)
rstats$clean_tweet =str_replace_all(rstats$clean_tweet,"[^[:graph:]]", " ")
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
jeopCorpus[[1]]$content
dtm <- TermDocumentMatrix(jeopCorpus)
tm_map(jeopCorpus, function(x) iconv(enc2utf8(x), sub = "byte"))
dtm <- TermDocumentMatrix(jeopCorpus)
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(removePunctuation) %>%
tm_map(tolower) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
jeopCorpus[[1]]$content
tm_map(jeopCorpus, function(x) iconv(enc2utf8(x), sub = "byte"))
dtm <- TermDocumentMatrix(jeopCorpus)
rstats$clean_tweet =str_replace_all(rstats$clean_tweet,"[^[:graph:]]", " ")
jeopCorpus <- Corpus(VectorSource(rstats$clean_tweet)) %>%
tm_map(removePunctuation) %>%
tm_map(tolower) %>%
tm_map(removeWords, stopwords('german')) %>%
tm_map(stemDocument)
jeopCorpus[[1]]$content
dtm <- TermDocumentMatrix(jeopCorpus)
tm_map(jeopCorpus, function(x) iconv(enc2utf8(x), sub = "byte"))
dtm <- TermDocumentMatrix(jeopCorpus)
jeopCorpus<-tm_map(jeopCorpus, function(x) iconv(enc2utf8(x), sub = "byte"))
dtm <- TermDocumentMatrix(jeopCorpus)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
wordcloud2(demoFreq, figPath = "C:/Users/Thomas/Documents/R/twitter/schweiz.jpg")
wordcloud2(demoFreq, figPath = "schweiz.jpg")
wordcloud2(demoFreq, figPath = "schweiz.jpeg")
wordcloud2(demoFreq, figPath = "schweiz.png")
wordcloud2(demoFreq, figPath = "schweiz.png")
wordcloud2(d, figPath = "schweiz.png")
wordcloud2(dtm, figPath = "schweiz.png")
wordcloud2(d, figPath = "schweiz.png")
wordcloud2(d, figPath = "schweiz.png")
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
wordcloud2(d, figPath = "schweiz.png")
wordcloud2(d, figPath = "swiss.png")
wordcloud2(d, figPath = "swiss.jpg")
wordcloud2(d, figPath = "swiss.jpg")
wordcloud2(d, figPath = "swiss.jpg")
demoFreq
d
wordcloud2(d, figPath = "swiss.jpg")
wordcloud2(d, figPath = "swiss.jpg")
wordcloud2(demoFreq, figPath = "swiss.jpg")
letterCloud(d,"#NOBILLAG")
library(wordcloud2)
library(tm)
library(wordcloud)
library(SnowballC)
library("textcat")
library("stringr")
library(tidyverse)
letterCloud(d,"#NOBILLAG")
letterCloud(d,"NOBILLAG")
View(d)
?mean
library(pacman)
p_load(tidyverse,igraph,ggraph,rtweet,hrbrthemes)
library(showtext)
## Loading Google fonts (http://www.google.com/fonts)
font_add_google("Raleway", "raleway")
showtext_auto()
rstats <-readRDS("rstatssrf1301.RDS")youtu
rstats <-readRDS("rstatssrf1301.RDS")
# glimpse(rstats)
#
# filter(rstats, retweet_count > 0) %>%
#   select(text, mentions_screen_name, retweet_count) %>%
#   mutate(text = substr(text, 1, 30)) %>%
#   unnest()
#
#
# filter(rstats, str_detect(text, "(RT|via)((?:[[:blank:]:]\\W*@\\w+)+)")) %>%
#   select(text, mentions_screen_name, retweet_count) %>%
#   mutate(extracted = str_match(text, "(RT|via)((?:[[:blank:]:]\\W*@\\w+)+)")[,3]) %>%
#   mutate(text = substr(text, 1, 30)) %>%
#   unnest()
filter(rstats, retweet_count > 5) %>%
select(screen_name, mentions_screen_name) %>%
unnest(mentions_screen_name) %>%
filter(!is.na(mentions_screen_name)) %>%
graph_from_data_frame() -> rt_g
ggplot(data_frame(y=degree_distribution(rt_g), x=1:length(y))) +
geom_segment(aes(x, y, xend=x, yend=0), color="slateblue") +
scale_y_continuous(expand=c(0,0), trans="sqrt") +
labs(x="Degree", y="Density (sqrt scale)", title="#rstats Retweet Degree Distribution")
# theme_ipsum_rc(grid="Y", axis="x")
V(rt_g)$node_label <- unname(ifelse(degree(rt_g)[V(rt_g)] > 25, names(V(rt_g)), ""))
V(rt_g)$node_size <- unname(ifelse(degree(rt_g)[V(rt_g)] > 25, degree(rt_g), 0))
# gg1 <-
ggraph(rt_g, layout = 'linear', circular = TRUE) +
geom_edge_arc(edge_width=0.0755, aes(alpha=..index..)) +
geom_node_label(aes(label=node_label, size=node_size),
label.size=0, fill="#ffffff66", segment.colour="springgreen",
color="slateblue", repel=TRUE, fontface="bold") +
coord_fixed() +
scale_size_area(trans="sqrt") +
labs(title="Retweet Relationships", subtitle="Namen mit den meisten Retweets gelabelt.\nDarkers edges == more retweets. Node size == larger degree") +
theme_graph(base_family="raleway") +
theme(legend.position="none")
# gg1
ggsave("graph.png", units="cm",width = 20, height = 20)
#
install.packages("keras")
# Load in the keras package
library(keras)
# Install TensorFlow
install_tensorflow()
# Load in the keras package
library(keras)
# Install TensorFlow
install_tensorflow()
# Klassifizierung der User nach Pro / Kontra funktioniert fÃ¼r die Top-Twitterer relativ gut
userkontra <-tweetpredictall %>% group_by(user,predcat) %>% summarize(anteil=n()/sum(n())) %>% arrange(desc(anteil))
library(rtweet)
library(rtweet)
nobillag <- search_tweets("#nobillag", n=5000)
neinzunobillag <- search_tweets("#neinzunobillag", n=5000)
nonobillag <- search_tweets("#nonobillag", n=5000)
nobillag_date <-bind_rowds(nobillag,neinzunobillag,nonobillag) %>% distinct(status_id,keep_all=T)
saveRDS(nobillag_date,"nobillag_11022018.RDS")
blogdown::new_site(theme="digitalcraftsman/hugo-minimalist-theme")
blogdown::new_site(theme="digitalcraftsman/hugo-minimalist-theme")
shiny::runApp('abstweets/example')
runApp('abstweets')
devtools::install_github("JohnCoene/echarts")
devtools::install_github("JohnCoene/echarts4r")
shiny::runApp('abstweets')
runApp('abstweets')
runApp('abstweets')
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
devtools::install_github("tidyverse/ggplot2")
devtools::install_github("tidyverse/ggplot2")
require(ggplot2)
install.packages("rlang")
install.packages("rlang")
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
# Dataset (Motorization and "generalabonnement"-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#merge data with shapefile
mapdata <- merge(shape2, mfz_ga, by.x="id", by.y="bfs",all.x = TRUE, all.y = TRUE)
mapdata <- mapdata[order(mapdata$order),]
#how does the df look like?
mapdata[11300:11310,]
# ggplot mapping  # data layer
ggplot(data=mapdata,aes(x=long, y=lat, group=group)) +
geom_path(color='gray') +
coord_equal()+
geom_polygon(aes(fill=Anteil_GA),color = "white")+
theme_void()+
theme(legend.key.size = unit(1,"line"),
#GrÃ¶sse Legende
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
sfmap
library(gghighlight)
#library(devtools)
#install_github(statistikZH/statR)
library(statR)
#library(devtools)
install_github(statistikZH/statR)
#library(devtools)
devtools::install_github(statistikZH/statR)
#library(devtools)
devtools::install_github("statistikZH/statR")
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
# Dataset (Motorization and "generalabonnement"-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#merge data with shapefile
mapdata <- merge(shape2, mfz_ga, by.x="id", by.y="bfs",all.x = TRUE, all.y = TRUE)
mapdata <- mapdata[order(mapdata$order),]
#how does the df look like?
mapdata[11300:11310,]
# ggplot mapping  # data layer
ggplot(data=mapdata,aes(x=long, y=lat, group=group)) +
geom_path(color='gray') +
coord_equal()+
geom_polygon(aes(fill=Anteil_GA),color = "white")+
theme_void()+
theme(legend.key.size = unit(1,"line"),
#GrÃ¶sse Legende
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
sfmap
library(gghighlight)
#library(devtools)
# devtools::install_github("statistikZH/statR")
# library(statR)
gghighlight_point(gemdata, aes(mfzpro100ew,Anteil_GA),Anteil_GA > 7 & mfzpro100ew<600 |
mfzpro100ew>1000|
mfzpro100ew<700 & Anteil_GA<3|
mfzpro100ew>750 & Anteil_GA>9)+
geom_point(aes(size=Anzahl_Einw/1000, color=Anteil_HTA))+
# theme_stat()+
scale_size(name="Einwohner")+
labs(title="Motorisierungsgrad & Generalabonnements pro Gemeinde\n",
y="Anzahl GA pro 100 Einwohner",x="Motorfahrzeuge pro 1000 Einwohner")
# Shapefiles of the "HandlungsrÃ¤ume & Bauzonen"
handlungsraum<- st_read("geodata/handlungsraeume", stringsAsFactors = FALSE)
bauzonen <- st_read("geodata/bauzonen", stringsAsFactors = FALSE)
sfmap+
geom_sf(data=handlungsraum,fill=NA)+
geom_sf(data=bauzonen,alpha=0.5,color="white")+
coord_sf(datum = NA)
# #CRS CH LV95 -> swiss projection
# st_crs(gemeinden)<- "+init=epsg:2056"
#Projektion wgs 84 lat lng
# sbb <- st_transform(sbb,"+init=epsg:4326 +proj=somerc +lat_0=46.95240555555556 +lon_0=7.439583333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +units=m +no_defs")
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
# require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
?fortify
??fortify
library(pacman)
pacman::p_load(rgdal, rgeos,ggplot2)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
require(ggplot2)
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- ggplot2::fortify(shape, region='BFS')
library(pacman)
pacman::p_load(rgdal, rgeos,tidyverse)
#ggplot2 version?
# devtools::install_github("tidyverse/ggplot2")
#read shapefile with rgdal (in past i've regularly used maptools::readShapePoly) this results in a nested datastructure (lists within lists) which is hard to decompose
shape <- readOGR("geodata/Shape_detailliert_SEEN_2016/UP_GEMEINDEN_SEEN_2016_F.shp")
#fortify -> generates a dataframe which ggplot2 can handle from a spatial object
shape2 <- fortify(shape, region='BFS')
# Dataset (Motorization and "generalabonnement"-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#merge data with shapefile
mapdata <- merge(shape2, mfz_ga, by.x="id", by.y="bfs",all.x = TRUE, all.y = TRUE)
mapdata <- mapdata[order(mapdata$order),]
#how does the df look like?
mapdata[11300:11310,]
# ggplot mapping  # data layer
ggplot(data=mapdata,aes(x=long, y=lat, group=group)) +
geom_path(color='gray') +
coord_equal()+
geom_polygon(aes(fill=Anteil_GA),color = "white")+
theme_void()+
theme(legend.key.size = unit(1,"line"),
#GrÃ¶sse Legende
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
detach(tidyverse)
library(pacman)
detach(tidyverse)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
require(ggplot2)
library(pacman)
pacman::p_load(sp,sf,dplyr,ggplot2)
require(ggplot2)
#Shapefile (municipalities) - read_sf generates a tidy dataset, each row represents one municipality, the polygons are stored in a single special geo-variable, the 'geometry' column
gemeinden<- read_sf('geodata/Shape_detailliert_SEEN_2016', stringsAsFactors = FALSE)
# Dataset (Motorization and GA-owners)
mfz_ga <-readRDS("mfz_ga.rds")
#join data - the dataset-structure stays the same, just
gemdata <-gemeinden %>% left_join(mfz_ga, by=c("BFS"="bfs"))
=======
>>>>>>> Stashed changes
#how does the df with class "sf" look like?
head(gemdata)
# dataframe with class sf can easily be ploted to get an overview! not so with sp...
plot(gemdata)
#map with ggplot2
sfmap <-ggplot()+
geom_sf(data=gemdata,aes(fill=Anteil_GA),color = "white")+
coord_sf(datum = NA)+
theme_void()+#Koordinatennetz verbergen
theme(legend.key.size = unit(1,"line"),
legend.key.height= unit(0.5,"line"))+
scale_fill_continuous(name="GA pro 100 EW")+
labs(title="GA-Dichte im Kanton ZÃ¼rich",
subtitle="Anzahl Generalabonnemente pro 100 Einwohner",x="",y="")
#ggplot2 version?
devtools::install_github("tidyverse/ggplot2")
gsg <- gs_title("gsg_multi")
tweets <- gsg %>% gs_read()
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags
hashtags=str_extract_all(text, "#(.*?) "))
setwd("~/abstweets_cron")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#fÃ¼r datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
gsg <- gs_title("gsg_multi")
?gs_title
gs_ls("juni2018")
gs_ls("/juni2018/")
gs_title("gsg_mult")
gs_ls("gsg_mu")
gsg <-gs_ls("gsg_mu")
tweets <- gsg %>% gs_read()
gsg <-gs_title(gs_ls("gsg_mu"))
map(gsg$sheet_title,gs_title)
gsg <- gs_title("gsg_multi")
View(gsg)
gsgl <-map(gsg$sheet_title,gs_title)
View(gsgl)
gsg <-gs_ls("gsg_mu")
gsgl <-map(gsg$sheet_title,gs_title)
gsgl <-map_df(gsg$sheet_title,gs_title)
gsgl <-map(gsg$sheet_title,gs_title)
tweets <- gsgl %>% gs_read()
tweets <- gsgl[1] %>% gs_read()
gsgl[1]
tweets <- gsgl[[1]] %>% gs_read()
length(gsgl)
dataframe <-map_df(gsgl[[1:length(gsgl)]],gs_reed)
dataframe <-map_df(gsgl[[1:length(gsgl)]],gs_read)
dataframe <-gsgl %>% map_df([[1:length(gsgl)]],gs_read)
dataframe <-gsgl %>% map_df(length(gsgl),gs_read)
dataframe <-gsgl %>% map_df(gsgl$sheet_title,gs_read)
dataframe <-gsgl %>% map_df(sheet_title,gs_read)
dataframe <-gsgl %>% map_df(length(gsgl$sheet_title),gs_read)
length(gsgl$sheet_title)
dataframe <-gsgl %>% map_df(length(gsgl),gs_read)
?map_df
dataframe <-gsgl %>% map_df(lgsgl[[1:length(gsgl)]],gs_read)
dataframe <-gsgl %>% map_df(gsgl[[1:length(gsgl)]],gs_read)
dataframe <-gsgl %>% map_df(gsgl[[length(gsgl)]],gs_read)
dataframe <-gsgl %>% map_df(gsgl[[]],gs_read)
gsgl[[1:length(gsgl)]] %>% gs_read()
dataframe <-gsgl %>% map_df(c(1:length(gsgl)),~gsgl[[.]] %>% gs_read())
c(1:length(gsgl)
1:length(gsgl)
length(gsgl)
1:length(gsgl)
dataframe <-gsgl %>% map_df(c(1:length(gsgl)),~ gsgl[[.x]] %>% gs_read())
dataframe <-gsgl %>% map_df(c(1:length(gsgl)), ~gsgl[[.x]] %>% gs_read())
gsgl <-map(gsg$sheet_title,gs_title)
dataframe <-gsgl %>% map_df(c(1:length(gsgl)), ~gsgl[[.x]]%>%gs_read())
dataframe <-gsgl %>% map_df(c(1:length(gsgl)), ~gsgl[[1]]%>%gs_read())
dataframe <-gsgl %>% map_dfr(c(1:length(gsgl)), ~gsgl[[1]]%>%gs_read())
dataframe <-gsgl %>% map_dfr(c(1:length(gsgl)), ~gsgl[[.x]]%>%gs_read())
dataframe <-gsgl %>% map_dfr( ~gsgl[[.x]]%>%gs_read())
dataframe <-gsgl %>% map(c(1:length(gsgl)), ~gsgl[[.x]]%>%gs_read())
dataframe <-gsgl %>% map(~gsgl[[.x]]%>%gs_read())
dataframe <-gsgl %>% map(~gsgl[.x]%>%gs_read())
gsgl[[1]] %>% gs_read()
tweets <- 1:3 %>% map %>% gsgl[[.]] %>% gs_read()
tweets <- 1:3 %>% map(~gsgl[[.]] %>% gs_read())
tweets <- 1:3 %>% map_dfr(~gsgl[[.]] %>% gs_read())
View(tweets)
tweets <- 1:3 %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
View(tweets)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
View(tweets)
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags
hashtags=str_extract_all(text, "#(.*?) "))
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
mutate(hashtags=str_extract_all(text, "#(.*?) "))
tweets_n_gsg<-tweets%>%
filter(hashtag>0) %>%
group_by(dmy) %>%
summarize(gsg=n())
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags
hashtags=str_extract_all(text, "#(.*?) "))
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
mutate(hashtags=str_extract_all(text, "#(.*?) "))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags
hashtags=str_extract_all(text, "#(.*?) "))
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
mutate(hashtags=str_extract_all(text, "#(.*?) "))
View(tweets)
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_mu")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags
hashtags=str_extract_all(text, "#(.*?) "))
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
mutate(hashtags=str_extract_all(text, "#(.*?) "))
View(tweets)
setwd("~/abstweets")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#fÃ¼r datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which contains gsg_mu in title : alle Vollgeld-sheets
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#files for shiny app tables
saveRDS(useractivity_gsg,"activity_gsg.rds")
saveRDS(userretweets_gsg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
# auskommentiert: im abstweets ordner (github) liegt die datei mit berechneter baseline!
# nobillag <- readRDS("allTweets_def.Rds")
#
# colnames(nobillag)<- c("screen_name","text","link","date")
#
# nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#
# #berechne countdown fÃ¼r Nobillag
# nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
#                                 group_by(dmynb) %>%
#                                 summarize(nb=n()) %>%
#                                 mutate(countdown=as.Date("2018-03-04")-dmynb)
# saveRDS(nobi,"nobillagbaseline.RDS")
nobi <-readRDS("nobillagbaseline.RDS")
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
# ASTG -> "Twitterreferendum #Sozialdetektive" ----------------------
#sheets which contains gsg_mu in title
sd <-gs_ls("Sozialdetektive")
#save google object ref
sd <-map(sd$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_sd<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_sd<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_sd<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_sd,"activity_sd.rds")
saveRDS(userretweets_sd,"retweets_sd.rds")
saveRDS(tweets_n_sd,"tweets_sd.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_sd.csv")
shiny::runApp()
runApp()
runApp()
setwd("~/abstweets")
# install.packages("googlesheets")
#load packages
pacman::p_load(googlesheets,tidyverse,lubridate,stringr,purrr)
pacman::p_load("cld2")
pacman::p_load("cld3")
#fÃ¼r datums-parsing
Sys.setenv(TZ='GMT')
Sys.setlocale("LC_ALL","English")
#load authentication token for googlesheets
gs_auth(token = "googlesheets_token.rds")
#sheets which contains gsg_mu in title : alle Vollgeld-sheets
vgi <-gs_ls("vgi_mehrere")
#save google object ref
vgl <-map(vgi$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(vgl) %>% map_dfr(~vgl[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_vg<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_vg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_vg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_vg,"activity_vg.rds")
saveRDS(userretweets_vg,"retweets_vg.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_vg.csv")
# Geldspielgesetz -----------------------------------
#sheets which containt gsg_mu in title
gsg <-gs_ls("gsg_multi")
#save google object ref
gsgl <-map(gsg$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(gsgl) %>% map_dfr(~gsgl[[.]] %>% gs_read(col_names = FALSE))
#column names
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
#hashtags that are used in the US in other contexts
corrupt_ht<-c("#gsg")
#detect language to filter tweets
tweets<- tweets %>% mutate(
cld2 = cld2::detect_language(text = text, plain_text = FALSE),
cld3 = cld3::detect_language(text = text)) %>%
filter(
#all the tweets without the hashtags that is used in other context
!(corrupt_ht %in%  hashtags)|
#all the tweets with the corrupt hashtags which are classified in a swiss language
corrupt_ht %in%  hashtags  & cld2 %in% c("de","fr","it") |
#all the tweets with the corrupt hashtags which are classified in a swiss language (different algo)
corrupt_ht %in%  hashtags  & cld3 %in% c("de","fr","it")
)
#check: which tweets get filtered out?
# anti<-tweets %>% anti_join(tweets2)
tweets_n_gsg<-tweets%>%
group_by(dmy) %>%
summarize(gsg=n())
tweetsagg <- tweets_n_gsg %>%
left_join(tweets_n_vg,by="dmy")
#Aktivste User
useractivity_gsg<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_gsg<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
# %>%
#   gather(vorlage,anzahl,-dmy)
#files for shiny app tables
saveRDS(useractivity_gsg,"activity_gsg.rds")
saveRDS(userretweets_gsg,"retweets_gsg.rds")
#single tweets for raw-data download
write.csv(tweets,"rawdata_gsg.csv")
# saveRDS(tweetsagg,"vg_gsg_without_baseline.rds")
#calculate countdown until vote
vg_gsg <-tweetsagg%>% mutate(countdown=as.Date("2018-06-10")-dmy)
#baseline: NO BILLAG ----------------------
# auskommentiert: im abstweets ordner (github) liegt die datei mit berechneter baseline!
# nobillag <- readRDS("allTweets_def.Rds")
#
# colnames(nobillag)<- c("screen_name","text","link","date")
#
# nobillag$date<- as.POSIXct(strptime(nobillag$date,"%B %d, %Y at %I:%M%p"))
#
# #berechne countdown fÃ¼r Nobillag
# nobi <- nobillag %>% mutate(dmynb=as.Date(date)) %>%
#                                 group_by(dmynb) %>%
#                                 summarize(nb=n()) %>%
#                                 mutate(countdown=as.Date("2018-03-04")-dmynb)
# saveRDS(nobi,"nobillagbaseline.RDS")
nobi <-readRDS("nobillagbaseline.RDS")
#join data to no-billag baseline
vg_gsg <-vg_gsg %>% left_join(nobi) %>% filter(dmy>"2018-04-19")
saveRDS(vg_gsg,"vg_gsg.rds")
# ASTG -> "Twitterreferendum #Sozialdetektive" ----------------------
#sheets which contains gsg_mu in title
sd <-gs_ls("Sozialdetektive")
#save google object ref
sd <-map(sd$sheet_title,gs_title)
#save all the identified objects in dataframe
tweets <- 1:length(sd) %>% map_dfr(~sd[[.]] %>% gs_read(col_names = FALSE))
colnames(tweets)<- c("screen_name","text","link","date")
tweets$date<- as.POSIXct(strptime(tweets$date,"%B %d, %Y at %I:%M%p"))
tweets<-tweets%>%
mutate(year= year(date),
week= week(date),
dmy=as.Date(date),
#extrahiere User-handle welcher geretweetet wurde
rt_user=str_extract(text, "RT @(.*?):"),
#extrahiere hash-tags!
hashtags=tolower(str_extract_all(text, "#(.*?) |#(.*)")))
tweets_n_sd<-tweets %>%
group_by(dmy) %>%
summarize(vg=n())
#Aktivste User
useractivity_sd<- tweets %>%
group_by(screen_name) %>%
summarize(n=n())
#Meiste Retweets
userretweets_sd<-tweets %>%
mutate(rt_user=str_replace_all(rt_user,"RT |:","")) %>%
group_by(rt_user) %>%
summarize(n=n()) %>%
filter(!is.na(rt_user))
#files for shiny app tables
saveRDS(useractivity_sd,"activity_sd.rds")
saveRDS(userretweets_sd,"retweets_sd.rds")
saveRDS(tweets_n_sd,"tweets_sd.rds")
#file for rawdata download (single tweets)
write.csv(tweets,"rawdata_sd.csv")
runApp()
runApp()
runApp()
runApp()
<<<<<<< Updated upstream
=======
=======
shiny::runApp()
install.packages("echarts4r")
devtools::install_github("JohnCoene/echarts4r")
library(echarts4r)
library("echarts4r")
shiny::runApp()
tweets_sd<- readRDS("tweets_sd.rds")
View(tweets_sd)
>>>>>>> Stashed changes
shiny::runApp()
runApp()
runApp()
>>>>>>> Stashed changes
